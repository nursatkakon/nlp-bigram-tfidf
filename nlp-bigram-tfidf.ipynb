{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7fa7f40",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6826fe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that deals understanding & processing human language. Language models are one of the most important parts of Natural Language Processing. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6968c",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff26128",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "There are primarily two types of language models:\n",
    "* Statistical Language Models - These models use traditional statistical techniques like n-grams, Hidden Markov Models (HMM) and certain linguistic rules to learn the probability distribution of words.\n",
    "* Neural Language Models - These models surpass statistical language models in their effectiveness. They use different kinds of neural networks to model language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55816f9c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "Both 'TF-IDF' and 'n-grams' are used to prepare text documents for searching. They provide different indexing rules to find matching documents. In this shot, we will work on `bigram model` & `TF-IDF` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a94cb",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077ff180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install nltk\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ef966",
   "metadata": {},
   "source": [
    "Importing the corpuses for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TheStoryofAnHour-KateChopin.txt') as f:\n",
    "    corpusA = f.read()\n",
    "\n",
    "with open('TheStoryofTheSlyFox-Jeanine Pirro.txt') as f:\n",
    "    corpusB = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e4db5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b></b> \n",
    "Another problem with the text analysis is it does not account for noise. In other words, certain words are used to formulate sentences that do not add any specific meaning to the text. for example, the most commonly used word in the english language is the which represents 7% of the all words written or spoken. We could not make reduce anythiing about the text given the fact that it contain the word the. On the other hand, words like good and awesome could be used to determine whether a rating was positive or not. \n",
    "\n",
    "In the natural language processing, useless words are referred to as stop words. The python natural language toolkit library provides a list of english stop words.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877e0f6",
   "metadata": {},
   "source": [
    "Importing the english stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15eb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "print('stopwords:\\n', stopwords[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55351a10",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad829458",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    " As preprocessing, we removed all capitalization, special characters, and numbers from the text. In addition, remove the following common words (called `stopwords`).\n",
    "\n",
    "After tokenization, removed also tokens with length less than 2 We perform this step because if e.g. you have in the text words like “sister’s”, tokenization will result in meaningless tokens like “s” (Warning: this might lead to omitting words such as the personal pronoun “I” which could be undesirable in practice, but for this exercise it is okay).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesssing(corpus):\n",
    "    corpus0 = corpus.lower()                                                        # removd all capitalization   \n",
    "    corpus1 = re.sub('[^A-Za-z0-9]+', ' ', corpus0)                                 # remove special characters\n",
    "    corpus2 = ''.join([i for i in corpus1 if not i.isdigit()])                      # remove special numbers\n",
    "    words = [word for word in corpus2.split() if word.lower() not in stopwords]     # remove stopwords\n",
    "    corpus3 = \" \".join(words)  \n",
    "    corpus4 = list(corpus3.split(\" \"))                                              # tokenization\n",
    "    corpus5 = [x for x in corpus4 if len(x) >= 2]                                   # remove tokens with length<2 [e.g: “s”, “I”]\n",
    "    return corpus5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0eb0f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<b></b> \n",
    "Machine learning algorithm cannot work with the raw text directly. Rather the text must be converted into vectors of numbers. In nature language processing, a common technique for extracting features from text is to place all of the words that occur in the text in a bucket. This approach is called a bag of words model or BoW for short. It's referred to as a \"bag\" of words because any information about the structure of the sentence is lost. Here, we will get the `Bow` as `bagOfWordsA` & `bagOfWordsB` after preprocessing.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ff1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = preprocesssing(corpusA)\n",
    "bagOfWordsB = preprocesssing(corpusB)\n",
    "print('bagOfWordsA:\\n', bagOfWordsA[0:20])\n",
    "print('\\nbagOfWordsB:\\n', bagOfWordsB[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc87f",
   "metadata": {},
   "source": [
    "# What is Bigram model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280bdb4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "In this shot, I will be implementing `bigram model`. In this model, we find bigrams, which are two words coming together in the corpus (the entire collection of words/sentences). We use a `bigram model` to predict the conditional probability of the next word. To estimate bigram probabilities, we can use the following equation:\n",
    "\n",
    "$$ P(W_n| W_{n-1}) = \\frac {count(W_{n-1}, W_n)} {count(W_{n-1})} $$\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramsA = [(s1, s2) for s1, s2 in zip(bagOfWordsA, bagOfWordsA[1:])]\n",
    "number_of_bigramsA = len(bigramsA)\n",
    "print('Number of total bigrams: ', number_of_bigramsA)\n",
    "print('\\nBigramsA:\\n', bigramsA[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f6a21",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "Using a word bi-gram language model, what is the probability of the phrase ‘life might’ in corpusA?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_life = bagOfWordsA.count(\"life\")/len(bagOfWordsA)\n",
    "prob_might_given_life = bigramsA.count(('life', 'might'))/bagOfWordsA.count('life')\n",
    "prob_life_might = prob_life * prob_might_given_life\n",
    "print ('Probability of the phrase ‘life might’ is:', prob_life_might)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544528a",
   "metadata": {},
   "source": [
    "# What is TF-IDF?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51baa6df",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "`TF-IDF` is useful in many natural language processing applications. For example, Search Engines use `TF-IDF` to rank the relevance of a corpus for a query. `TF-IDF` is also employed in text classification, text summarization, and topic modeling.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87308b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "print('uniqueWords:\\n', list(uniqueWords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dbb563",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1      \n",
    "\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1 \n",
    "    \n",
    "print('numOfWordsA:\\n', sorted(numOfWordsA.items())[:10])\n",
    "print('\\nnumOfWordsB:\\n', sorted(numOfWordsB.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131f9331",
   "metadata": {},
   "source": [
    "# Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43e36c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "TF measures how frequently a term occurs in a corpus. Since every corpus is different in length, it is possible that a term would appear much more times in long corpuss than shorter ones. Thus, the term frequency is often divided by the corpus length (aka. the total number of terms in the corpus) as a way of normalization:\n",
    "   \n",
    "\n",
    "$$ tf_{(i,j)} = \\frac {n_{(i,j)}} {\\sum_k n_{(i,j)}} $$\n",
    "Every corpus has its own term frequency.   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988cdcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ab407",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "\n",
    "def take(n, iterable):\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "print('tfA:\\n', take(10, tfA.items()))\n",
    "print('\\ntfA:\\n', take(10, tfB.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce573d9",
   "metadata": {},
   "source": [
    "# Inverse corpus Frequency (IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43445702",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "IDF measures how important a term is. While computing TF, all terms are considered equally important. The log of the number of corpuss divided by the number of corpuss that contain the word w. Inverse data frequenct determines the weight of rare words across all corpuss in the corpus.\n",
    "    \n",
    "$$ idf(w) = log (\\frac{N}{df_t}) $$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 1)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "                \n",
    "                \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f45d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "print('idfs:\\n', take(30, idfs.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdc3e5",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb30b1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "The TF-IDF of a term is calculated by multiplying TF and IDF scores.\n",
    "    \n",
    "$$ w_{i,j} = tf_{i,j} * log(\\frac{N}{df_i}) $$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829e8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b786f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "df = df.sort_index(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d9514",
   "metadata": {},
   "source": [
    "Let's check the dataframe avoiding the null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eedb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(0, np.nan)\n",
    "df = df[df.columns[~df.isnull().all()]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a50ae5",
   "metadata": {},
   "source": [
    "# Using `TfidfVectorizer` from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89a26e9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "`TfidfVectorizer` converts a collection of raw documents to a matrix of TF-IDF features. It uses an in-memory vocabulary (as dict) to map the most frequent words to feature indices and hence compute a word occurrence frequency (sparse) matrix.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174e6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([corpusA, corpusB])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns = feature_names)\n",
    "df = df.sort_index(axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b846b8f",
   "metadata": {},
   "source": [
    "Let's check the dataframe avoiding the null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046852c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(0, np.nan)\n",
    "df = df[df.columns[~df.isnull().all()]]\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
